{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-07 21:37:17.990817: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Use Keras Neural Network Components. This will be used to construct the LSTM\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import L1L2\n",
    "\n",
    "#Import Tensorflow \n",
    "import tensorflow as tf\n",
    "\n",
    "#Import Pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensure Reproducability of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch and Set Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerically Encode the Activities\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATDIR = \"data/UCI HAR Dataset\"\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"gravity_acc_x\",\n",
    "    \"gravity_acc_y\",\n",
    "    \"gravity_acc_z\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test: 0 if we want to fetch the training signal data, 1 if we want to fetch the test signal data\n",
    "def fetch_raw_signal_data(train_test, signal):\n",
    "    file_path = DATDIR + f\"/{'train' if train_test == 0 else 'test'}/Inertial Signals/{signal}_{'train' if train_test == 0 else 'test'}.txt\"\n",
    "    return pd.read_csv(file_path, delim_whitespace = True, header = None)\n",
    "\n",
    "#Same purpose as previous function but, in this case, we are fetching ALL the raw signal data!\n",
    "#train_test: 0 if we want to fetch the training signal data, 1 if we want to fetch the test signal data\n",
    "def fetch_all_raw_signals(train_test):\n",
    "    data_raw_signals = []\n",
    "    for signal in SIGNALS:\n",
    "        raw_signal_data = fetch_raw_signal_data(train_test, signal).to_numpy()\n",
    "        data_raw_signals.append(raw_signal_data)\n",
    "    \n",
    "    data_raw_signals = np.array(data_raw_signals)\n",
    "    return np.transpose(data_raw_signals, axes = (1, 2, 0))\n",
    "\n",
    "#Fetch Labels\n",
    "#train_test: 0 if we want to fetch the training signal data, 1 if we want to fetch the test signal data\n",
    "def fetch_labels(train_test):\n",
    "    file_path = DATDIR + f\"/{'train' if train_test == 0 else 'test'}/y_{'train' if train_test == 0 else 'test'}.txt\"\n",
    "    return pd.get_dummies(pd.read_csv(file_path, delim_whitespace=True, header = None)[0]).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7352 128 9\n",
      "6\n",
      "(7352, 128, 9) (2947, 128, 9) (7352, 6) (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = fetch_all_raw_signals(0), fetch_all_raw_signals(1), fetch_labels(0), fetch_labels(1)\n",
    "\n",
    "N = X_train.shape[0]\n",
    "T = X_train.shape[1]\n",
    "D1 = X_train.shape[2]\n",
    "\n",
    "print(N, T, D1)\n",
    "\n",
    "n_classes = Y_train.shape[1]\n",
    "print(n_classes)\n",
    "\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, D2, H, L):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Take the Raw Inertial Samples and Embed them in a Higher Dimension\n",
    "        self.convolution_series = nn.Sequential(\n",
    "            nn.Conv1d(D1, D2, 1), nn.GELU(),\n",
    "            nn.Conv1d(D2, D2, 1), nn.GELU(),\n",
    "            nn.Conv1d(D2, D2, 1), nn.GELU(),\n",
    "            nn.Conv1d(D2, D2, 1), nn.GELU())\n",
    "        \n",
    "        #Generate an Embedding for each position in the input sequence\n",
    "        self.position_embedding = nn.Parameter(torch.randn(T + 1, 1, D2))\n",
    "        \n",
    "        #Transformer Encoder[Comprised of Self Multi-Head Attention and FC Network]\n",
    "        self.encoder_layer = TransformerEncoderLayer(d_model = D2, nhead = H, dim_feedforward = 2 * D2, dropout = 0.1, activation = \"relu\")\n",
    "        \n",
    "        #Stack together those EncoderLayers to form the TransformerEncoder\n",
    "        #The TransformerEncoder will use L TransformerEncoderLayer units\n",
    "        self.transformer_encoder = TransformerEncoder(self.encoder_layer, L)\n",
    "        \n",
    "        #Class Token\n",
    "        self.cls_token = nn.Parameter(torch.zeros((1, D2)), requires_grad=True)\n",
    "        \n",
    "        #FC Stuff at the End\n",
    "        self.classifier_head = nn.Sequential(\n",
    "            nn.LayerNorm(D2),\n",
    "            nn.Linear(D2, D2//4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(D2//4,  n_classes)\n",
    "        )\n",
    "    \n",
    "    #Run forward pass given X(input data)\n",
    "    #Shape of X: N x T x D1\n",
    "    def forward(self, X):\n",
    "        #First do the higher dimensional embedding\n",
    "        X = self.convolution_series(X.transpose(1, 2)).permute(2, 0, 1)\n",
    "        \n",
    "        #Generate class token\n",
    "        cls_token = self.cls_token.unsqueeze(1).repeat(1, X.shape[1], 1)\n",
    "        X = torch.cat([cls_token, X])\n",
    "        \n",
    "        #Add Positional Embedding\n",
    "        X += self.position_embedding\n",
    "        \n",
    "        #Run through the Transformer Encoder\n",
    "        target = self.transformer_encoder(X)[0]\n",
    "        \n",
    "        #Get Class Probabilities\n",
    "        target = self.classifier_head(target)\n",
    "        \n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raviraghavan/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:1, BATCH: 1, Loss:1.8632832765579224\n",
      "EPOCH:1, BATCH: 2, Loss:1.855995774269104\n",
      "EPOCH:1, BATCH: 3, Loss:1.8368000984191895\n",
      "EPOCH:1, BATCH: 4, Loss:1.820155382156372\n",
      "EPOCH:1, BATCH: 5, Loss:1.8175426721572876\n",
      "EPOCH:1, BATCH: 6, Loss:1.8289141654968262\n",
      "EPOCH:1, BATCH: 7, Loss:1.8049559593200684\n",
      "EPOCH:1, BATCH: 8, Loss:1.8083213567733765\n",
      "EPOCH:1, BATCH: 9, Loss:1.8101487159729004\n",
      "EPOCH:1, BATCH: 10, Loss:1.8173434734344482\n",
      "EPOCH:1, BATCH: 11, Loss:1.818457007408142\n",
      "EPOCH:1, BATCH: 12, Loss:1.8097198009490967\n",
      "EPOCH:1, BATCH: 13, Loss:1.811043381690979\n",
      "EPOCH:1, BATCH: 14, Loss:1.7848436832427979\n",
      "EPOCH:1, BATCH: 15, Loss:1.8020832538604736\n",
      "EPOCH:2, BATCH: 1, Loss:1.7952358722686768\n",
      "EPOCH:2, BATCH: 2, Loss:1.7960096597671509\n",
      "EPOCH:2, BATCH: 3, Loss:1.773695945739746\n",
      "EPOCH:2, BATCH: 4, Loss:1.7498066425323486\n",
      "EPOCH:2, BATCH: 5, Loss:1.7402819395065308\n",
      "EPOCH:2, BATCH: 6, Loss:1.6967148780822754\n",
      "EPOCH:2, BATCH: 7, Loss:1.6765378713607788\n",
      "EPOCH:2, BATCH: 8, Loss:1.7031128406524658\n",
      "EPOCH:2, BATCH: 9, Loss:1.5985413789749146\n",
      "EPOCH:2, BATCH: 10, Loss:1.5806522369384766\n",
      "EPOCH:2, BATCH: 11, Loss:1.5716792345046997\n",
      "EPOCH:2, BATCH: 12, Loss:1.5463500022888184\n",
      "EPOCH:2, BATCH: 13, Loss:1.5326513051986694\n",
      "EPOCH:2, BATCH: 14, Loss:1.4723862409591675\n",
      "EPOCH:2, BATCH: 15, Loss:1.4773247241973877\n",
      "EPOCH:3, BATCH: 1, Loss:1.4814373254776\n",
      "EPOCH:3, BATCH: 2, Loss:1.479596734046936\n",
      "EPOCH:3, BATCH: 3, Loss:1.4639604091644287\n",
      "EPOCH:3, BATCH: 4, Loss:1.4923039674758911\n",
      "EPOCH:3, BATCH: 5, Loss:1.432382345199585\n",
      "EPOCH:3, BATCH: 6, Loss:1.3758411407470703\n",
      "EPOCH:3, BATCH: 7, Loss:1.4488521814346313\n",
      "EPOCH:3, BATCH: 8, Loss:1.3590030670166016\n",
      "EPOCH:3, BATCH: 9, Loss:1.4122971296310425\n",
      "EPOCH:3, BATCH: 10, Loss:1.3814949989318848\n",
      "EPOCH:3, BATCH: 11, Loss:1.424843430519104\n",
      "EPOCH:3, BATCH: 12, Loss:1.4123016595840454\n",
      "EPOCH:3, BATCH: 13, Loss:1.3304390907287598\n",
      "EPOCH:3, BATCH: 14, Loss:1.4526112079620361\n",
      "EPOCH:3, BATCH: 15, Loss:1.455980658531189\n",
      "EPOCH:4, BATCH: 1, Loss:1.395066499710083\n",
      "EPOCH:4, BATCH: 2, Loss:1.3104947805404663\n",
      "EPOCH:4, BATCH: 3, Loss:1.3692642450332642\n",
      "EPOCH:4, BATCH: 4, Loss:1.4083354473114014\n",
      "EPOCH:4, BATCH: 5, Loss:1.340006947517395\n",
      "EPOCH:4, BATCH: 6, Loss:1.3672504425048828\n",
      "EPOCH:4, BATCH: 7, Loss:1.3872321844100952\n",
      "EPOCH:4, BATCH: 8, Loss:1.375623106956482\n",
      "EPOCH:4, BATCH: 9, Loss:1.370831847190857\n",
      "EPOCH:4, BATCH: 10, Loss:1.380774736404419\n",
      "EPOCH:4, BATCH: 11, Loss:1.3411654233932495\n",
      "EPOCH:4, BATCH: 12, Loss:1.3829567432403564\n",
      "EPOCH:4, BATCH: 13, Loss:1.3184467554092407\n",
      "EPOCH:4, BATCH: 14, Loss:1.3359251022338867\n",
      "EPOCH:4, BATCH: 15, Loss:1.3872545957565308\n",
      "EPOCH:5, BATCH: 1, Loss:1.352269172668457\n",
      "EPOCH:5, BATCH: 2, Loss:1.343702793121338\n",
      "EPOCH:5, BATCH: 3, Loss:1.3766324520111084\n",
      "EPOCH:5, BATCH: 4, Loss:1.358177900314331\n",
      "EPOCH:5, BATCH: 5, Loss:1.3105199337005615\n",
      "EPOCH:5, BATCH: 6, Loss:1.3666313886642456\n",
      "EPOCH:5, BATCH: 7, Loss:1.337235689163208\n",
      "EPOCH:5, BATCH: 8, Loss:1.3654530048370361\n",
      "EPOCH:5, BATCH: 9, Loss:1.3395414352416992\n",
      "EPOCH:5, BATCH: 10, Loss:1.3762962818145752\n",
      "EPOCH:5, BATCH: 11, Loss:1.349135160446167\n",
      "EPOCH:5, BATCH: 12, Loss:1.3634719848632812\n",
      "EPOCH:5, BATCH: 13, Loss:1.2843295335769653\n",
      "EPOCH:5, BATCH: 14, Loss:1.3196309804916382\n",
      "EPOCH:5, BATCH: 15, Loss:1.2468856573104858\n",
      "EPOCH:6, BATCH: 1, Loss:1.301235556602478\n",
      "EPOCH:6, BATCH: 2, Loss:1.3211404085159302\n",
      "EPOCH:6, BATCH: 3, Loss:1.3644206523895264\n",
      "EPOCH:6, BATCH: 4, Loss:1.3013930320739746\n",
      "EPOCH:6, BATCH: 5, Loss:1.3069078922271729\n",
      "EPOCH:6, BATCH: 6, Loss:1.35990571975708\n",
      "EPOCH:6, BATCH: 7, Loss:1.2857228517532349\n",
      "EPOCH:6, BATCH: 8, Loss:1.3429324626922607\n",
      "EPOCH:6, BATCH: 9, Loss:1.352440357208252\n",
      "EPOCH:6, BATCH: 10, Loss:1.273911714553833\n",
      "EPOCH:6, BATCH: 11, Loss:1.338032841682434\n",
      "EPOCH:6, BATCH: 12, Loss:1.3960291147232056\n",
      "EPOCH:6, BATCH: 13, Loss:1.2753266096115112\n",
      "EPOCH:6, BATCH: 14, Loss:1.3563634157180786\n",
      "EPOCH:6, BATCH: 15, Loss:1.4232066869735718\n",
      "EPOCH:7, BATCH: 1, Loss:1.3014657497406006\n",
      "EPOCH:7, BATCH: 2, Loss:1.3012866973876953\n",
      "EPOCH:7, BATCH: 3, Loss:1.3224434852600098\n",
      "EPOCH:7, BATCH: 4, Loss:1.2877304553985596\n",
      "EPOCH:7, BATCH: 5, Loss:1.357701301574707\n",
      "EPOCH:7, BATCH: 6, Loss:1.3205668926239014\n",
      "EPOCH:7, BATCH: 7, Loss:1.3795559406280518\n",
      "EPOCH:7, BATCH: 8, Loss:1.3638488054275513\n",
      "EPOCH:7, BATCH: 9, Loss:1.3208810091018677\n",
      "EPOCH:7, BATCH: 10, Loss:1.3232146501541138\n",
      "EPOCH:7, BATCH: 11, Loss:1.2890300750732422\n",
      "EPOCH:7, BATCH: 12, Loss:1.3398475646972656\n",
      "EPOCH:7, BATCH: 13, Loss:1.3387415409088135\n",
      "EPOCH:7, BATCH: 14, Loss:1.284870982170105\n",
      "EPOCH:7, BATCH: 15, Loss:1.3978890180587769\n",
      "EPOCH:8, BATCH: 1, Loss:1.3339061737060547\n",
      "EPOCH:8, BATCH: 2, Loss:1.329390048980713\n",
      "EPOCH:8, BATCH: 3, Loss:1.3067948818206787\n",
      "EPOCH:8, BATCH: 4, Loss:1.3006843328475952\n",
      "EPOCH:8, BATCH: 5, Loss:1.3418877124786377\n",
      "EPOCH:8, BATCH: 6, Loss:1.377211332321167\n",
      "EPOCH:8, BATCH: 7, Loss:1.3107216358184814\n",
      "EPOCH:8, BATCH: 8, Loss:1.2829837799072266\n",
      "EPOCH:8, BATCH: 9, Loss:1.3634538650512695\n",
      "EPOCH:8, BATCH: 10, Loss:1.351095199584961\n",
      "EPOCH:8, BATCH: 11, Loss:1.3168764114379883\n",
      "EPOCH:8, BATCH: 12, Loss:1.3225529193878174\n",
      "EPOCH:8, BATCH: 13, Loss:1.3217518329620361\n",
      "EPOCH:8, BATCH: 14, Loss:1.2962522506713867\n",
      "EPOCH:8, BATCH: 15, Loss:1.3060123920440674\n",
      "EPOCH:9, BATCH: 1, Loss:1.2953935861587524\n",
      "EPOCH:9, BATCH: 2, Loss:1.4132112264633179\n",
      "EPOCH:9, BATCH: 3, Loss:1.3535076379776\n",
      "EPOCH:9, BATCH: 4, Loss:1.2749661207199097\n",
      "EPOCH:9, BATCH: 5, Loss:1.3616505861282349\n",
      "EPOCH:9, BATCH: 6, Loss:1.3221901655197144\n",
      "EPOCH:9, BATCH: 7, Loss:1.334649682044983\n",
      "EPOCH:9, BATCH: 8, Loss:1.349198818206787\n",
      "EPOCH:9, BATCH: 9, Loss:1.2775959968566895\n",
      "EPOCH:9, BATCH: 10, Loss:1.310842514038086\n",
      "EPOCH:9, BATCH: 11, Loss:1.3276790380477905\n",
      "EPOCH:9, BATCH: 12, Loss:1.3290753364562988\n",
      "EPOCH:9, BATCH: 13, Loss:1.3114955425262451\n",
      "EPOCH:9, BATCH: 14, Loss:1.313277006149292\n",
      "EPOCH:9, BATCH: 15, Loss:1.308293342590332\n",
      "EPOCH:10, BATCH: 1, Loss:1.3293896913528442\n",
      "EPOCH:10, BATCH: 2, Loss:1.3415955305099487\n",
      "EPOCH:10, BATCH: 3, Loss:1.2403219938278198\n",
      "EPOCH:10, BATCH: 4, Loss:1.3227357864379883\n",
      "EPOCH:10, BATCH: 5, Loss:1.3392544984817505\n",
      "EPOCH:10, BATCH: 6, Loss:1.3435792922973633\n",
      "EPOCH:10, BATCH: 7, Loss:1.3356502056121826\n",
      "EPOCH:10, BATCH: 8, Loss:1.2908984422683716\n",
      "EPOCH:10, BATCH: 9, Loss:1.3676632642745972\n",
      "EPOCH:10, BATCH: 10, Loss:1.3319613933563232\n",
      "EPOCH:10, BATCH: 11, Loss:1.2949961423873901\n",
      "EPOCH:10, BATCH: 12, Loss:1.3182075023651123\n",
      "EPOCH:10, BATCH: 13, Loss:1.3559342622756958\n",
      "EPOCH:10, BATCH: 14, Loss:1.3272466659545898\n",
      "EPOCH:10, BATCH: 15, Loss:1.2889539003372192\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 512\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "model = TransformerModel(D2 = 12, H = 4, L = 6)\n",
    "\n",
    "#Set up loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the optimizer and scheduler\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "#Set number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    batch_idx = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "\n",
    "        # Zero the gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optim.step()\n",
    "    \n",
    "        print(f\"EPOCH:{epoch+1}, BATCH: {batch_idx+1}, Loss:{loss.item()}\")\n",
    "        batch_idx += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 34.88%\n"
     ]
    }
   ],
   "source": [
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "# Create a DataLoader\n",
    "test_loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Initialize variables for evaluation\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute predictions\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, actual = torch.max(targets, 1)\n",
    "\n",
    "        # Update evaluation statistics\n",
    "        total_samples += targets.size(0)\n",
    "        total_correct += (predicted == actual).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
